# training_pipeline_a
from utils.logging_utils import create_logger
import pandas as pd

class jobPostIingest:
    """
    Ingestion of job posts, feeding in only the ones that haven't yet been processed.
    """

    def __init__(self, job_descr_path: str, gpt_output_path: str, log_level:str):
        """                
        Args:
        job_descr_path (str): Path to JSON file containing job descriptions.
        gpt_output_path (str): Path to CSV file containing ChatGPT responses.
        log_level (str): log level (INFO, DEBUG, etc)
        """
        self.job_descr_path = job_descr_path
        self.gpt_output_path = gpt_output_path
        self.logger = create_logger(log_level = log_level, log_name = 'jp_ingestion_log')

    def ingest_jp(self):
        """
        Reads job descriptions from a JSON file at `job_descr_path` and appends any new responses generated by ChatGPT to the end of the file. 
        Removes duplicate rows based on all columns except 'salaries' and 'skill_summary'.

        
        Args:
            job_descr_path (str): Path to JSON file containing job descriptions.
            gpt_output_path (str): Path to CSV file containing ChatGPT responses.
        
        Returns:
            pandas.DataFrame: A merged DataFrame containing unique rows of job descriptions and ChatGPT responses.
        """
        if self.job_descr_path.exists():
            df = pd.read_json(self.job_descr_path)
            self.logger.info('reading in raw job post data')
        column_mask = (df.columns != 'salaries') & (df.columns != f'skill_summary')
        df = df.drop_duplicates(subset = df.columns[column_mask])
        if self.gpt_output_path.exists():
            df_gpt = pd.read_csv(self.gpt_output_path)
            self.logger.info('reading in previously summarized job post data')
            df = pd.concat([df,df_gpt]).drop_duplicates(subset = df.columns[column_mask], keep = False)
        return df