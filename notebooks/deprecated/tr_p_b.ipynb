{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.logging_utils import create_logger\n",
    "from yaml import safe_load\n",
    "from pathlib import Path\n",
    "from ingestion.database import VectorDataBase, PostgresDatabase\n",
    "from utils.logging_utils import create_logger\n",
    "from preprocessing.spacy_prep import SpacyPrep\n",
    "from modeling_clusterization.embedding import FastTextModel\n",
    "from modeling_clusterization.gpt_models import ChatGpt, Gpt4All\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 01:16:18] {application_questions_pipeline_log:45} INFO - running application_questions_pipeline.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with open('src/universal_config.yaml', 'r') as file:\n",
    "with open('universal_config.yaml', 'r') as file:\n",
    "\n",
    "    cf = safe_load(file)\n",
    "\n",
    "cf_db_p = cf['database']['postgres']\n",
    "cf_db_q = cf['database']['qdrant']\n",
    "\n",
    "\n",
    "# with open('src/application_questions_config.yaml', 'r') as file:\n",
    "with open('application_questions_config.yaml', 'r') as file:\n",
    "\n",
    "    cf_appq = safe_load(file)\n",
    "\n",
    "\n",
    "# parent_folder_path = Path().cwd()\n",
    "parent_folder_path = Path(cf['parent_path'])\n",
    "print(parent_folder_path)\n",
    "data_path = parent_folder_path.parent / 'data' \n",
    "app_questions_data_path = data_path / 'app_questions' \n",
    "embed_model_path = app_questions_data_path / 'artifacts' / 'fasttext.model'\n",
    "fasttext_training_it_dataset_path = app_questions_data_path / 'prep_data' / 'prep_sentences_50K.csv'\n",
    "\n",
    "\n",
    "# paths to gpt binaries and generated output csv files\n",
    "cf_gpt_model_to_use = cf_appq['gpt_model_constants']['gpt_model_to_use']\n",
    "model_path = parent_folder_path.parent / 'data' / cf['gpt_model_constants']['model_path']\n",
    "# model_path = parent_folder_path / 'data' / cf['gpt_model_constants']['model_path']  ## for .py file\n",
    "gpt_model_str =  cf['gpt_model_constants'][cf_gpt_model_to_use]['gpt_model_str'] #or use gpt4all constants\n",
    "\n",
    "# strings to generate prompts\n",
    "system_string = cf_appq['gpt_model_constants']['default_prompt_strings']['system_string']\n",
    "user_string = cf_appq['gpt_model_constants']['default_prompt_strings']['user_string']\n",
    "substring_to_replace = cf_appq['gpt_model_constants']['default_prompt_strings']['substring_to_replace']\n",
    "different_answer_user_string = cf_appq['gpt_model_constants']['default_prompt_strings']['different_answer_user_string']\n",
    "\n",
    "# other gpt constants  - use .env variables, so that these can be altered easily\n",
    "hist_qa_return_limit = int(os.getenv('HIST_QA_RETURN_LIMIT'))\n",
    "gpt_answer_return_limit = int(os.getenv('GPT_ANSWER_RETURN_LIMIT'))\n",
    " \n",
    "\n",
    "# logging constants and pipeline logger\n",
    "log_level = cf['utils']['log_level']\n",
    "tpb_logger = create_logger(log_level, log_name = 'application_questions_pipeline_log')\n",
    "tpb_logger.info('running application_questions_pipeline.py')\n",
    "\n",
    "postgres_db = PostgresDatabase()\n",
    "spc = SpacyPrep()\n",
    "embed_model = FastTextModel(embed_model_path)\n",
    "qdrant_db = VectorDataBase( cf_db_q['questions']['table_name'],\n",
    "                                cf_db_q['vec_size'])\n",
    "\n",
    "# if cf_gpt_model_to_use == 'chatgpt':\n",
    "#     chatgpt_api_timing_delay =  cf['gpt_model_constants'][cf_gpt_model_to_use]['chatgpt_api_timing_delay']\n",
    "#     gpt_model = ChatGpt(gpt_model_str, model_path, chatgpt_api_timing_delay, log_level)\n",
    "# else:\n",
    "#     gpt_model = Gpt4All(gpt_model_str, model_path, log_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('d:/project/ai_job_autopilot/gitlab/ai_core/src')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path().cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 00:29:42] {PostgresDatabase:259} INFO - Postgres database core_backend_stage_db has been connected \n",
      "[08-09 00:29:42] {VectorDataBase:75} INFO - Qdrant database has been connected\n",
      "[08-09 00:29:46] {FastTextModel:93} INFO - Loaded fast text model from disk\n",
      "[08-09 00:29:46] {application_session.py:51} INFO - Application session initialized\n",
      "[08-09 00:29:46] {SpacyPrep:86} INFO - cleaning words to a list words\n",
      "[08-09 00:29:46] {SpacyPrep:104} INFO - cleaning list of sentences to a list of lists of words\n",
      "[08-09 00:29:46] {FastTextModel:131} INFO - Converting strings of text to vectors via fasttext model\n",
      "[08-09 00:29:46] {VectorDataBase:167} INFO - Qdrant table questions has been queried\n",
      "d:\\project\\ai_job_autopilot\\gitlab\\ai_core\\src\\ingestion\\database.py:392: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  self.df_a = psql.read_sql(query, self.conn)\n",
      "[08-09 00:29:47] {PostgresDatabase:396} INFO - fetched the text of the answer\n",
      "d:\\project\\ai_job_autopilot\\gitlab\\ai_core\\src\\ingestion\\database.py:392: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  self.df_a = psql.read_sql(query, self.conn)\n",
      "[08-09 00:29:48] {PostgresDatabase:396} INFO - fetched the text of the answer\n",
      "d:\\project\\ai_job_autopilot\\gitlab\\ai_core\\src\\ingestion\\database.py:392: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  self.df_a = psql.read_sql(query, self.conn)\n",
      "[08-09 00:29:48] {PostgresDatabase:396} INFO - fetched the text of the answer\n",
      "[08-09 00:29:48] {application_session.py:151} INFO - Found and retrieved a matching historical question-answer pair(s)\n",
      "[08-09 00:29:48] {application_session.py:152} INFO - Question(s): dict_values(['How many years of experience do you have in Team Leadership?', 'Do you have experience in a customer-facing role?', 'How many years of work experience do you have with Technical Support?'])\n",
      "[08-09 00:29:48] {application_session.py:153} INFO - Answer(s): dict_keys(['answer to question: How many years of experience do you have in Team Leadership?', 'Yes, in my previous role at XYZ Retail, I interacted with customers daily, assisting with their queries, handling transactions, and ensuring a positive shopping experience.', 'answer to question: How many years of work experience do you have with Technical Support?'])\n",
      "[08-09 00:29:48] {application_session.py:154} INFO - New question: Do you have experience in customer service?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  d:\\\\\\\\project\\\\\\\\ai_job_autopilot\\\\\\\\gitlab\\\\\\\\ai_core\\\\\\\\data\\\\\\\\job_posts\\\\\\\\artifacts\\\\\\\\gpt_models\\\\ggml-gpt4all-j-v1.3-groovy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 00:30:27] {gpt_models-gpt4All_log:65} INFO - attempting to generate gpt4all predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found model file at  d:\\\\\\\\project\\\\\\\\ai_job_autopilot\\\\\\\\gitlab\\\\\\\\ai_core\\\\\\\\data\\\\\\\\job_posts\\\\\\\\artifacts\\\\\\\\gpt_models\\\\ggml-gpt4all-j-v1.3-groovy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 00:31:04] {gpt_models-gpt4All_log:65} INFO - attempting to generate gpt4all predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 00:31:05] {application_session.py:106} INFO - GPT model generated a tuned answer\n",
      "[08-09 00:31:05] {application_session.py:107} INFO - Answer:  Yes, I have experience in customer service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  d:\\\\\\\\project\\\\\\\\ai_job_autopilot\\\\\\\\gitlab\\\\\\\\ai_core\\\\\\\\data\\\\\\\\job_posts\\\\\\\\artifacts\\\\\\\\gpt_models\\\\ggml-gpt4all-j-v1.3-groovy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 00:31:51] {gpt_models-gpt4All_log:65} INFO - attempting to generate gpt4all predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 00:31:52] {application_session.py:106} INFO - GPT model generated a tuned answer\n",
      "[08-09 00:31:52] {application_session.py:107} INFO - Answer:  I have experience in customer service, and my previous role at XYZ Retail involved interacting with customers daily, handling transactions and ensuring a positive shopping experience.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  d:\\\\\\\\project\\\\\\\\ai_job_autopilot\\\\\\\\gitlab\\\\\\\\ai_core\\\\\\\\data\\\\\\\\job_posts\\\\\\\\artifacts\\\\\\\\gpt_models\\\\ggml-gpt4all-j-v1.3-groovy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 00:32:45] {gpt_models-gpt4All_log:65} INFO - attempting to generate gpt4all predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-09 00:32:46] {application_session.py:106} INFO - GPT model generated a tuned answer\n",
      "[08-09 00:32:46] {application_session.py:107} INFO - Answer:  I have experience in customer service, and my previous role at XYZ Retail involved interacting with customers daily, handling transactions and ensuring a positive shopping experience.\n"
     ]
    }
   ],
   "source": [
    "from sessions.application_session import ApplicationSession\n",
    "\n",
    "\n",
    "app_session = ApplicationSession(postgres_db, spc, \n",
    "                embed_model, qdrant_db,\n",
    "                gpt_model, fasttext_training_it_dataset_path,\n",
    "                log_level)\n",
    "app_session.initialize_session()\n",
    "hist_answer_question_dict, ret_list = app_session.query_tune_answer(user_string, system_string, substring_to_replace, \n",
    "                        different_answer_user_string, new_question = 'Do you have experience in customer service?',\n",
    "                        hist_qa_return_limit=3, gpt_answer_return_limit = 3\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_answer_question_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist_answer_question_dict\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hist_answer_question_dict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-core-RiE3R9BB-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
