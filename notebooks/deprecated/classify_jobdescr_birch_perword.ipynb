{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\slavr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\slavr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HELPER METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters=10\n",
    "# birch_clusterizer = cluster.Birch(n_clusters=None, threshold=0.2)\n",
    "# for i in range(0,len(X),20):\n",
    "#     birch_clusterizer.partial_fit(X[i:i+20])\n",
    "\n",
    "# birch_clusterizer.set_params(n_clusters=n_clusters)\n",
    "# birch_clusterizer.partial_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def tsne_3d_plot(X, birch_clusterizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Reduce X to 3 dimensions and plot all points over 3 axes\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame(data = X)\n",
    "    X_df['label'] = birch_clusterizer.subcluster_labels_\n",
    "\n",
    "\n",
    "    tsne_model = TSNE(n_components=3, learning_rate='auto',\n",
    "                    init='random', perplexity=3)\n",
    "                    \n",
    "    projections = tsne_model.fit_transform(X)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        projections, x=0, y=1, z=2,\n",
    "        color=X_df.label, labels={'color': 'label'}\n",
    "    )\n",
    "    fig.update_traces(marker_size=8)\n",
    "\n",
    " \n",
    "\n",
    "# tsne_3d_plot(X, birch_clusterizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "def matpotlib_cluster_plot(X, birch_clusterizer):\n",
    "    # Use all colors that matplotlib provides by default.\n",
    "    colors_ = cycle(colors.cnames.keys())\n",
    "\n",
    "    labels = birch_clusterizer.subcluster_labels_\n",
    "    centroids = birch_clusterizer.subcluster_centers_\n",
    "    n_clusters = np.unique(labels).size\n",
    "\n",
    "    print(\"n_clusters : %d\" % n_clusters)\n",
    "    for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):\n",
    "        mask = labels == k\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], c=\"w\", edgecolor=col, marker=\".\", alpha=0.5)\n",
    "        plt.scatter(this_centroid[0], this_centroid[1], marker=\"+\", c=\"k\", s=155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2word(model,row):\n",
    "    \n",
    "    word = model.wv.most_similar(positive=[row.values])[0][0]\n",
    "    return word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_to_df_relabel_avg(clusterizer, model):\n",
    "    \"\"\"\n",
    "    create a dataframe with labels as averaged cluster centroid\n",
    "    \"\"\"\n",
    "    labels = clusterizer.subcluster_labels_\n",
    "    centroids = clusterizer.subcluster_centers_\n",
    "\n",
    "    df_c = pd.DataFrame(centroids)\n",
    "    df_c['label'] = labels\n",
    "\n",
    "    df_c = df_c.groupby(by='label').mean()\n",
    "\n",
    "    df_c['label_txt'] = df_c.apply(lambda row: vec2word(model, row), axis=1)\n",
    "\n",
    "    return df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_to_df_relabel_first(clusterizer, model):\n",
    "    \"\"\"\n",
    "    create a dataframe with labels as first cluster centroid\n",
    "    \"\"\"\n",
    "    labels = clusterizer.subcluster_labels_\n",
    "    centroids = clusterizer.subcluster_centers_\n",
    "    n_clusters = np.unique(labels).size\n",
    "\n",
    "    u, idx_start = np.unique(labels, return_counts=False, return_index=True)\n",
    "    df_c = pd.DataFrame(centroids[idx_start])\n",
    "    labels=[]\n",
    "    for i in range(n_clusters):\n",
    "        labels.append(model.wv.most_similar(positive=[df_c.loc[i,:].values])[0][0]\n",
    "        )\n",
    "    df_c['label_txt'] = labels\n",
    "\n",
    "    return df_c\n",
    "    \n",
    "def centroid_to_df(birch_clusterizer):\n",
    "    \"\"\"\n",
    "    create a dataframe with \n",
    "    \"\"\"\n",
    "    labels = birch_clusterizer.subcluster_labels_\n",
    "    centroids = birch_clusterizer.subcluster_centers_\n",
    "    n_clusters = np.unique(labels).size\n",
    "\n",
    "    u, idx_start = np.unique(labels, return_counts=False, return_index=True)\n",
    "    df_c = pd.DataFrame(centroids[idx_start])\n",
    "    df_c['label_txt'] = np.unique(labels)\n",
    "\n",
    "    return df_c\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "def plotly_3d_centroid_plot(df_c):\n",
    "    \"\"\"\n",
    "    generate 3d plot of cluster centers\n",
    "    \"\"\"\n",
    "    tsne_model = TSNE(n_components=3, learning_rate='auto',\n",
    "                    init='random', perplexity=3)\n",
    "\n",
    "    label_col_mask = df_c.columns[df_c.columns != 'label_txt']\n",
    "    df_proj = tsne_model.fit_transform(df_c[label_col_mask])\n",
    "    df_proj = pd.DataFrame(df_proj)\n",
    "    df_proj['label_txt'] = df_c['label_txt'].values\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        df_proj, x=0, y=1, z=2,\n",
    "        color=df_proj.label_txt, labels={'color': 'label_txt'}\n",
    "    )\n",
    "    fig.update_traces(marker_size=8)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectorize(list_of_docs, model, clusterizer):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            clusterizer.partial_fit(vectors)\n",
    "            features.append(vectors)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features, clusterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA NGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('d:/project/ai_job_autopilot/gitlab')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_folder_path = Path().cwd().parent\n",
    "parent_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('d:/project/ai_job_autopilot/gitlab/original_data/jd1.json')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descr_path = parent_folder_path / 'original_data/jd1.json'\n",
    "\n",
    "job_descr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(job_descr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would like to present to you a new job opportunity and I think you may find it interesting.\n",
      "\n",
      "If you are interested kindly send the following documents to by Thursday, April 06 at 01:00 PM EST if that interests you and matches your profile.\n",
      "\n",
      "Without mandatory documents, we cannot submit a candidate.\n",
      "• Updated Resume in word format (Mandatory)\n",
      "• Skills Matrix and References (Mandatory)\n",
      "• Expected hourly rate (Mandatory)\n",
      "\n",
      "Job Title: RQ05180 - DevOPS/Cloud Engineer - Senior\n",
      "\n",
      "Client: Ministry of Health\n",
      "\n",
      "Work Location: 5700 Yonge Street, Toronto, Ontario, Hybrid\n",
      "\n",
      "Estimated Start Date: 2023-06-02\n",
      "\n",
      "Estimated End Date: 2024-03-29\n",
      "\n",
      "#Business Days: 207.00\n",
      "\n",
      "Extension: Probable after the initial mandate\n",
      "\n",
      "Hours per day or Week: 7.25 hours per day\n",
      "\n",
      "Security Level: CRJMC\n",
      "\n",
      "If you are interested to learn more about this opportunity or ifnot, please feel free to send over any names or forward this email to anyone who may be interested. Please check it out on our career site.\n",
      "\n",
      "(Click above for Skill... matrix and for more details)\n",
      "\n",
      "Must haves:\n",
      "• AWS technologies\n",
      "• Knowledge of Salesforce solution and MuleSoft API integration\n",
      "• Devops & CI/DC applications\n",
      "\n",
      "Description:\n",
      "• Provide expertise in AWS cloud platform.\n",
      "• Provide design, build and support for cloud environments to create solutions for Public Health domain.\n",
      "• Provide expertise on Linux operating system like Red-Hat including installation, troubleshooting, hardening and scripting to automate.\n",
      "• The role will monitor and assess the performance of applications in a cloud environment to ensure solutions are available\n",
      "• Participate to create, test and implement safeguards to maintain data integrity and protect against unauthorized access\n",
      "• Regularly review production logs, providing analysis & suggestions to implement measures in a proactive manner\n",
      "• Hands-on experience with microservices and distributed application architecture, such as containers, Kubernetes, and/or serverless technology\n",
      "• Provide detailed documentation for system design, integration, configuration, development and implementation of related activities\n",
      "• Develop and maintain system design models, technical documentation and specifications\n",
      "• Produce integration plans, inputs into configuration and development project.\n",
      "\n",
      "Experience And Skill Set Requirements\n",
      "\n",
      "Technical Skills – 50 points\n",
      "\n",
      "10+ years of experience in IT field:\n",
      "• Creation, management, and maintenance of AWS & related cloud-based production system.\n",
      "• Deployment and automation of packages forcloud-based system\n",
      "• Ensuring availability, performance, security, and scalability of AWS & related production systems.\n",
      "• Excellent knowledge in managing AWS resources and configuring Amazon VPC, AWS Firewall, Amazon Elastic Load Balancing, auto-scaling, AWS IAM, Amazon EC2, Amazon S3, Amazon API Gateway, AWS Lambda, Amazon Aurora DB, Amazon Redshift, Active MQ, AWS CloudTrail, AWS CloudWatch, and other services in the AWS family.\n",
      "• Knowledge of the following Amazon Web Services (AWS) technologies – Amazon Connect, Amazon Pinpoint, Amazon DynamoDB, Amazon Kinesis Data Streams and Amazon Polly\n",
      "• 5+ years working experience of Unix/Linux operating systems like Solaris and Red-Hat including installation, troubleshooting, hardening and scripting to automate\n",
      "• 3+ years with administration of Kubernetes and Docker container solutions\n",
      "• Proven implementation of cloud security models, particularly identity, network, and encryption\n",
      "• Knowledge of Gitlab or other DevOps tools and CI/CD integration\n",
      "• Knowledge of Salesforce solution and MuleSoft API integration\n",
      "• Provision of critical system security by leveraging best practices and prolific cloud security solutions.\n",
      "\n",
      "Core Skills And Experience - 30 Points\n",
      "\n",
      "10+ years of experience with:\n",
      "• Production environment troubleshooting and tuning to improve application performance\n",
      "• Developing and maintaining system design models, technical documentation and specifications\n",
      "• Experience setting up development environments and mechanism using tools such as JIRA, Confluence, Maven and Jenkins or similar tools\n",
      "• Experience in scripting languages like Python, Bash, PHP, Java, JavaScript, Node, etc.\n",
      "• Experience in build tools like Git, Ansible, Chef, Puppet etc. for continuous integration\n",
      "• Knowledge of container-based virtualization technology like Docker\n",
      "• Integration experience in building and using APIs\n",
      "• Experience applying industry web, architectural and security standards and best practices\n",
      "• Experience in mobile device management for various versions of cellular and tablets\n",
      "• Providing recommendations for architecture and process improvements.\n",
      "• Definition and deployment of systems for metrics, logging, and monitoring on AWS and related platform.\n",
      "• Designing, maintenance and management of tools for automation of different operational processes.\n",
      "• Evaluation of new technology alternatives and vendor products.\n",
      "\n",
      "Public Sector/Healthcare Experience - 5 Points\n",
      "• 5+ years of experience working with federal/provincial/broader public-sector healthcare providers\n",
      "• Knowledge of Public Sector Enterprise Architecture artifacts (or similar), processes and practices, and ability to produce technical documentation that comply with industry standard practices\n",
      "• In-depth knowledge of industry standard such as Project Management Institute (PMI) and Public Sector I&IT project management methodologies.\n",
      "• Knowledge and experience with Public Sector Health related projects\n",
      "• Knowledge and understanding of Ministry policy and IT project approval processes and requirements\n",
      "• Experience adopting and adhering to Public Sector Unified I&IT Project Methodology, Public Sector Enterprise Architecture and Public Sector Gating process, and Public Sector Standard Systems Development Methodologies\n",
      "• Experience with large complex IT Health-related projects.\n",
      "\n",
      "General Skills - 15 Points\n",
      "• Proven technical leadership skills with ability to identify areas for improvement, and recommend solutions\n",
      "• Exceptional analytical, problem solving and decision-making skills\n",
      "• Demonstrated strong interpersonal, verbal and written communication, and presentation skills\n",
      "• Proven troubleshooting and critical thinking experience\n",
      "• Demonstrated ability to apply strong listening skills to facilitate issue resolution\n",
      "• Effective consulting skills to engage with all stakeholders with proven track record for building strong working relationships\n",
      "• Strong interpersonal, facilitation and negotiation skills with ability to build rapport with stakeholders and drive negotiations to a successful outcome\n",
      "• Excellent customer service skills, including tact and diplomacy to ensure client needs are managed effectively\n",
      "• A motivated, flexible, detail-oriented and creative team player with perseverance, excellent organization and multi-tasking abilities, and a proven track record for meeting strict deadlines.\n",
      "\n",
      "Powered by JazzHR\n",
      "\n",
      "L1MPqU904c\n"
     ]
    }
   ],
   "source": [
    "print(df['description'][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = set(stopwords.words(\"english\") + [\"job\", \"title\", \"qualifications\", \"requirements\"])\n",
    "\n",
    "df[\"tokens\"] = df[\"description\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "tokenized_docs = df[\"tokens\"].values\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(parent_folder_path / 'prep_data' / 'models' / \"word2vec.model\")\n",
    "model = gensim.models.FastText.load(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterizer = Birch(n_clusters=None, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1208, (359, 200))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_docs, clusterizer = vectorize(tokenized_docs, model=model, clusterizer = clusterizer)\n",
    "len(vectorized_docs), vectorized_docs[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Birch(n_clusters=200, threshold=0.2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Birch</label><div class=\"sk-toggleable__content\"><pre>Birch(n_clusters=200, threshold=0.2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Birch(n_clusters=200, threshold=0.2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = 200\n",
    "clusterizer.set_params(n_clusters=n_clusters)\n",
    "clusterizer.partial_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = centroid_to_df_relabel_avg(clusterizer, model)\n",
    "\n",
    "plotly_3d_centroid_plot(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ticket', 'viral', 'notices', 'die', 'labor', 'webapi', 'pcg', 'sprint', 'rmo', 'back', 'twitter', 'confident', 'mockups', 'markup', 'construction', 'issue', 'asynchronous', 'relationship_building', 'bdo', 'bcne', 'rd', 'webchat', 'cto', 'biologic', 'waste', 'profiles', 'coso', 'balancing', 'emailing', 'aluminium', 'wfh', 'reconciliations', 'creo', 'scope', 'event', 'logo', 'restful', 'positive', 'vsphere', 'turnaround', 'brokers', 'web', 'country', 'spices', 'direct', 'vitro', 'curry', 'landscape', 'metadata', 'selenium_webdriver', 'neural', 'dispatch', 'rabbitmq', 'awareness', 'drive', 'dba', 'hedging', 'lgd', 'installations', 'game', 'language', 'phone', 'finalization', 'payment', 'pivot', 'documents', 'rockwell', 'qa', 'ey', 'adserving', 'wanted', 'wellness', 'promise', 'shell', 'clinical_operation', 'evaluation', 'gathering', 'wfm', 'feature', 'tied', 'contracts', 'queries', 'django', 'ensuring', 'per', 'enhancements', 'fico', 'display_drivers', 'headcount', 'vulnerability', 'rlc', 'intrusion', 'minimum', 'assessments', 'kafka', 'factiva', 'debugging', 'character', 'proactive', 'get', 'iscsi', 'history', 'stability', 'anywhere', 'authentication', 'customization', 'recruiter', 'salesforce', 'retirement', 'executives', 'ssms', 'jobs', 'university', 'meeting', 'base_sas', 'emulators', 'provider', 'springs', 'ibm', 'builders', 'ccie', 'scenarios', 'graphite', 'kinesis', 'consultative', 'outsourcing', 'redux', 'weaving', 'team', 'requisition', 'assistance', 'tutoring', 'openmp', 'closure', 'escalations', 'cobol', 'oriented', 'modelsim', 'stationary', 'manufacturing', 'compilers', 'problem', 'second', 'estate', 'interfaces', 'feedback', 'black', 'ilt', 'wmq', 'bank', 'teleperformance', 'coordination', 'public', 'xcode', 'against', 'immigration', 'sshr', 'docker', 'developers', 'coffee', 'package', 'mathematics', 'customer_focus', 'printers', 'raising', 'procurement', 'fill', 'resource', 'php', 'exchanger', 'classical_mechanics', 'leave', 'genetics', 'maven', 'javascript', 'governance', 'improvements', 'geographical', 'computer', 'months', 'problem_analysis', 'subject', 'administrator', 'coaching', 'beverage', 'health', 'connectivity', 'ensemble', 'utility', 'technically', 'consultancy', 'crm', 'french', 'models', 'policy', 'data_pump', 'clearance', 'mapping', 'desk', 'operation']\n"
     ]
    }
   ],
   "source": [
    "print(df_c.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model.wv.__getitem__('senior')\n",
    "# labelind_2_labeltext(clusterizer, labelinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "def print_cluster_centroid_text_doc_ind(df, ind):\n",
    "    labelinds = clusterizer.predict(vectorized_docs[ind])\n",
    "    labelinds_counter = Counter(labelinds)\n",
    "    count_list = labelinds_counter.most_common(10)\n",
    "    print(df['description'][ind])\n",
    "    print('------------')\n",
    "    print([(df_c.loc[count,'label_txt'], freq) for count,freq in count_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requisition ID: 174456\n",
      "\n",
      "Join a purpose driven winning team, committed to results, in an inclusive and high-performing culture.\n",
      "\n",
      "The Team\n",
      "\n",
      "We are part of the GBME – Global Banking Markets and Engineering - Data Platform team who acts as a key business functionality within the bank leveraged by regulatory and non-regulatory systems. Data platform currently supports 100+ source systems in terms of development and solutioning using some of the key cloud technologies like Hadoop / Kafka / NiFi / Spark / Elasticsearch / Minio & Dremio.\n",
      "\n",
      "The role:\n",
      "\n",
      "The GBM Data Platform DevOps is responsible for providing onboarding solutions to the above-mentioned technology stack as well as maintain stability and integrity of the data platform. This role is a dedicated resource to support regulatory / compliance data platform onboarding guidance and production support.\n",
      "\n",
      "To achieve this, your responsibilities include:\n",
      "\n",
      "• Design Recommendations: Work very closely with project teams on understanding the... business requirements, technical limitations, and user insights, and help facilitate the teams to design their process according to the appropriate technology stack.\n",
      "\n",
      "• Onboarding: Frequent interactions with project teams in terms of design analysis and guiding them to ideal solutions using the above technology stack.\n",
      "\n",
      "• Production support: Monitor system inconsistencies, comment on issues reported that involve design and suggest solutions for issue resolutions to the respective project teams.\n",
      "\n",
      "Do you have the skills that will enable you to succeed in this role?\n",
      "• 6+ years of DevOps / SRE experience.\n",
      "• Must have skills:\n",
      "• 4+ years of hands-on experience with Cloud Technologies\n",
      "• Hands on experience with CI/CD workflows\n",
      "• 4+ years of experience working on Unix.\n",
      "• Proficient with Elasticsearch\n",
      "• Technical Skills preferred: -\n",
      "• Kubernetes\n",
      "• Rancher\n",
      "• Docker\n",
      "• Hadoop\n",
      "• NiFi/Spark\n",
      "• Ownership & Technical leadership\n",
      "• Background in Hadoop or Big Data would be beneficial.\n",
      "• Excellent written and verbal communication skills Experience working in an Agile environment\n",
      "• Comfortable working in a self-guided manner\n",
      "• Bachelor's degree in a technical field such as Computer Science, Computer Engineering, or related field\n",
      "• Capital Markets and Banking experience is an asset\n",
      "\n",
      "Why Scotiabank?\n",
      "• We have an inclusive and collaborative working environment that encourages creativity, curiosity, and celebrates success!\n",
      "• We provide you with the tools and technology needed to create beautiful customer experiences\n",
      "• You'll get to work with and learn from diverse industry leaders, who have hailed from top technology companies around the world\n",
      "• Dress codes don't apply here, being comfortable does\n",
      "• We offer a competitive total rewards package that includes a base salary, a performance bonus, company matching programs (on pension & profit sharing), generous vacation, personal & sick days, personal development funding, maternity leave top-up, parental leave and much more.\n",
      "\n",
      "#Li-Hybrid\n",
      "\n",
      "#ScotiaTechnology\n",
      "\n",
      "#ScotiaRed\n",
      "\n",
      "#CapitalMarketTech\n",
      "\n",
      "Location(s): Canada : Ontario : Toronto\n",
      "\n",
      "Scotiabank is a leading bank in the Americas. Guided by our purpose: \"for every future\", we help our customers, their families and their communities achieve success through a broad range of advice, products and services, including personal and commercial banking, wealth management and private banking, corporate and investment banking, and capital markets.\n",
      "\n",
      "At Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted\n",
      "------------\n",
      "[('confident', 23), ('candidate', 9), ('bank', 8), ('parsing', 7), ('neural', 6), ('brokers', 6), ('attitude', 6), ('cto', 6), ('improvements', 6), ('one', 6)]\n"
     ]
    }
   ],
   "source": [
    "print_cluster_centroid_text_doc_ind(df, ind=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1208, 6)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would like to present to you a new job opportunity and I think you may find it interesting.\n",
      "\n",
      "If you are interested kindly send the following documents to by Thursday, April 06 at 01:00 PM EST if that interests you and matches your profile.\n",
      "\n",
      "Without mandatory documents, we cannot submit a candidate.\n",
      "• Updated Resume in word format (Mandatory)\n",
      "• Skills Matrix and References (Mandatory)\n",
      "• Expected hourly rate (Mandatory)\n",
      "\n",
      "Job Title: RQ05180 - DevOPS/Cloud Engineer - Senior\n",
      "\n",
      "Client: Ministry of Health\n",
      "\n",
      "Work Location: 5700 Yonge Street, Toronto, Ontario, Hybrid\n",
      "\n",
      "Estimated Start Date: 2023-06-02\n",
      "\n",
      "Estimated End Date: 2024-03-29\n",
      "\n",
      "#Business Days: 207.00\n",
      "\n",
      "Extension: Probable after the initial mandate\n",
      "\n",
      "Hours per day or Week: 7.25 hours per day\n",
      "\n",
      "Security Level: CRJMC\n",
      "\n",
      "If you are interested to learn more about this opportunity or ifnot, please feel free to send over any names or forward this email to anyone who may be interested. Please check it out on our career site.\n",
      "\n",
      "(Click above for Skill... matrix and for more details)\n",
      "\n",
      "Must haves:\n",
      "• AWS technologies\n",
      "• Knowledge of Salesforce solution and MuleSoft API integration\n",
      "• Devops & CI/DC applications\n",
      "\n",
      "Description:\n",
      "• Provide expertise in AWS cloud platform.\n",
      "• Provide design, build and support for cloud environments to create solutions for Public Health domain.\n",
      "• Provide expertise on Linux operating system like Red-Hat including installation, troubleshooting, hardening and scripting to automate.\n",
      "• The role will monitor and assess the performance of applications in a cloud environment to ensure solutions are available\n",
      "• Participate to create, test and implement safeguards to maintain data integrity and protect against unauthorized access\n",
      "• Regularly review production logs, providing analysis & suggestions to implement measures in a proactive manner\n",
      "• Hands-on experience with microservices and distributed application architecture, such as containers, Kubernetes, and/or serverless technology\n",
      "• Provide detailed documentation for system design, integration, configuration, development and implementation of related activities\n",
      "• Develop and maintain system design models, technical documentation and specifications\n",
      "• Produce integration plans, inputs into configuration and development project.\n",
      "\n",
      "Experience And Skill Set Requirements\n",
      "\n",
      "Technical Skills – 50 points\n",
      "\n",
      "10+ years of experience in IT field:\n",
      "• Creation, management, and maintenance of AWS & related cloud-based production system.\n",
      "• Deployment and automation of packages forcloud-based system\n",
      "• Ensuring availability, performance, security, and scalability of AWS & related production systems.\n",
      "• Excellent knowledge in managing AWS resources and configuring Amazon VPC, AWS Firewall, Amazon Elastic Load Balancing, auto-scaling, AWS IAM, Amazon EC2, Amazon S3, Amazon API Gateway, AWS Lambda, Amazon Aurora DB, Amazon Redshift, Active MQ, AWS CloudTrail, AWS CloudWatch, and other services in the AWS family.\n",
      "• Knowledge of the following Amazon Web Services (AWS) technologies – Amazon Connect, Amazon Pinpoint, Amazon DynamoDB, Amazon Kinesis Data Streams and Amazon Polly\n",
      "• 5+ years working experience of Unix/Linux operating systems like Solaris and Red-Hat including installation, troubleshooting, hardening and scripting to automate\n",
      "• 3+ years with administration of Kubernetes and Docker container solutions\n",
      "• Proven implementation of cloud security models, particularly identity, network, and encryption\n",
      "• Knowledge of Gitlab or other DevOps tools and CI/CD integration\n",
      "• Knowledge of Salesforce solution and MuleSoft API integration\n",
      "• Provision of critical system security by leveraging best practices and prolific cloud security solutions.\n",
      "\n",
      "Core Skills And Experience - 30 Points\n",
      "\n",
      "10+ years of experience with:\n",
      "• Production environment troubleshooting and tuning to improve application performance\n",
      "• Developing and maintaining system design models, technical documentation and specifications\n",
      "• Experience setting up development environments and mechanism using tools such as JIRA, Confluence, Maven and Jenkins or similar tools\n",
      "• Experience in scripting languages like Python, Bash, PHP, Java, JavaScript, Node, etc.\n",
      "• Experience in build tools like Git, Ansible, Chef, Puppet etc. for continuous integration\n",
      "• Knowledge of container-based virtualization technology like Docker\n",
      "• Integration experience in building and using APIs\n",
      "• Experience applying industry web, architectural and security standards and best practices\n",
      "• Experience in mobile device management for various versions of cellular and tablets\n",
      "• Providing recommendations for architecture and process improvements.\n",
      "• Definition and deployment of systems for metrics, logging, and monitoring on AWS and related platform.\n",
      "• Designing, maintenance and management of tools for automation of different operational processes.\n",
      "• Evaluation of new technology alternatives and vendor products.\n",
      "\n",
      "Public Sector/Healthcare Experience - 5 Points\n",
      "• 5+ years of experience working with federal/provincial/broader public-sector healthcare providers\n",
      "• Knowledge of Public Sector Enterprise Architecture artifacts (or similar), processes and practices, and ability to produce technical documentation that comply with industry standard practices\n",
      "• In-depth knowledge of industry standard such as Project Management Institute (PMI) and Public Sector I&IT project management methodologies.\n",
      "• Knowledge and experience with Public Sector Health related projects\n",
      "• Knowledge and understanding of Ministry policy and IT project approval processes and requirements\n",
      "• Experience adopting and adhering to Public Sector Unified I&IT Project Methodology, Public Sector Enterprise Architecture and Public Sector Gating process, and Public Sector Standard Systems Development Methodologies\n",
      "• Experience with large complex IT Health-related projects.\n",
      "\n",
      "General Skills - 15 Points\n",
      "• Proven technical leadership skills with ability to identify areas for improvement, and recommend solutions\n",
      "• Exceptional analytical, problem solving and decision-making skills\n",
      "• Demonstrated strong interpersonal, verbal and written communication, and presentation skills\n",
      "• Proven troubleshooting and critical thinking experience\n",
      "• Demonstrated ability to apply strong listening skills to facilitate issue resolution\n",
      "• Effective consulting skills to engage with all stakeholders with proven track record for building strong working relationships\n",
      "• Strong interpersonal, facilitation and negotiation skills with ability to build rapport with stakeholders and drive negotiations to a successful outcome\n",
      "• Excellent customer service skills, including tact and diplomacy to ensure client needs are managed effectively\n",
      "• A motivated, flexible, detail-oriented and creative team player with perseverance, excellent organization and multi-tasking abilities, and a proven track record for meeting strict deadlines.\n",
      "\n",
      "Powered by JazzHR\n",
      "\n",
      "L1MPqU904c\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Most representative terms per cluster (based on centroids):\")\n",
    "# for i in range(50):\n",
    "#     tokens_per_cluster = \"\"\n",
    "#     most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "#     for t in most_representative:\n",
    "#         tokens_per_cluster += f\"{t[0]} \"\n",
    "#     print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cluster = 29\n",
    "# most_representative_docs = np.argsort(\n",
    "#     np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    "# )\n",
    "# for d in most_representative_docs[:3]:\n",
    "#     print(df['description'][d])\n",
    "#     print(\"-------------\")\n",
    "#     most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[test_cluster]], topn=5)\n",
    "#     print(most_representative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobauto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
